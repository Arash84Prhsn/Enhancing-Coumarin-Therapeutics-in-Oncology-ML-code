{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372d9abb",
   "metadata": {},
   "source": [
    "# ML model trainings are done in different code cells of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af5f6a",
   "metadata": {},
   "source": [
    "## XGBOOST REGRESSOR ON ALL COUMARINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd40058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error,\n",
    "    median_absolute_error, r2_score, make_scorer\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# Paths for dataset, trained models, and evaluation reports.\n",
    "# ============================================================\n",
    "DATA_PATH = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/Total_Data.csv'\n",
    "RESULT_DIR = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/GridSearchingResults/Models/XGBOOST_REGRESSOR'\n",
    "REPORT_DIR = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/GridSearchingResults/Reports/XGBOOST_REGRESSOR'\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD & PREPARE DATA\n",
    "# Includes: loading dataset, column selection, cleaning, label encoding\n",
    "# ============================================================\n",
    "data = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
    "required_columns = ['Coumarin Type', 'Cancer Type', 'Coumarin Dose', 'Time', 'Viability']\n",
    "data = data[required_columns].dropna()\n",
    "\n",
    "# Convert dose to numeric and restrict doses ≤ 400\n",
    "data['Coumarin Dose'] = pd.to_numeric(data['Coumarin Dose'], errors='coerce')\n",
    "data = data[data['Coumarin Dose'] <= 400]\n",
    "\n",
    "# Encode cancer types for ML models\n",
    "cancerType_Encoder = LabelEncoder();\n",
    "coumarin_Encoder = LabelEncoder();\n",
    "data['Cancer Type'] = cancerType_Encoder.fit_transform(data['Cancer Type'])\n",
    "data['Coumarin Type'] = coumarin_Encoder.fit_transform(data['Coumarin Type'])\n",
    "\n",
    "# Some compounds have an additional 96h measurement\n",
    "allowed_times = [24, 48, 72, 96];\n",
    "auraptene_allowed_time = [24, 48, 72];\n",
    "data = data[\n",
    "    ((data['Coumarin Type'] != \"Auraptene\") & (data['Time'].isin(allowed_times))) |\n",
    "    ((data['Coumarin Type'] == \"Auraptene\") & (data['Time'].isin(auraptene_allowed_time)))\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# GRID SEARCHING FOR OPTIMAL HYPER PARAMETERS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n=====================================<GRID SEARCHING>=====================================\")\n",
    "\n",
    "\n",
    "xgb_paramGrid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'gamma': [0, 1],\n",
    "    'reg_lambda': [1, 5],\n",
    "    'reg_alpha': [0, 1],\n",
    "}\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(100, 1200, 100),\n",
    "    'max_depth': np.arange(2, 12, 1),\n",
    "    'min_child_weight': np.arange(1, 10, 1),\n",
    "    'learning_rate': np.linspace(0.005, 0.3, 20),\n",
    "    'subsample': np.linspace(0.5, 1.0, 6),\n",
    "    'colsample_bytree': np.linspace(0.5, 1.0, 6),\n",
    "    'gamma': np.linspace(0, 10, 11),\n",
    "    'reg_alpha': np.logspace(-3, 1, 10),   # L1\n",
    "    'reg_lambda': np.logspace(-1, 2, 10),  # L2\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X = data[['Coumarin Type', 'Coumarin Dose', 'Time', 'Cancer Type']]\n",
    "y = data['Viability']\n",
    "# {'colsample_bytree': 1.0, 'gamma': 1, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 300, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 1.0}\n",
    "# BEST R2 SCORE ON : 0.6486041733242749\n",
    "xgb_gridSearcher = GridSearchCV(estimator=XGBRegressor(random_state=42,),\n",
    "                                            param_grid=xgb_paramGrid,\n",
    "                                            scoring='r2',\n",
    "                                            verbose=1,\n",
    "                                            cv=kf,\n",
    "                                            n_jobs=-1,\n",
    "                                            )\n",
    "\n",
    "xgb_gridSearcher.fit(X,y)\n",
    "bestParameters_xgb = xgb_gridSearcher.best_params_;\n",
    "bestR2Score_xgb = xgb_gridSearcher.best_score_;\n",
    "\n",
    "print(f\"\\nBEST R2 SCORE ON : {bestR2Score_xgb}\")\n",
    "print(bestParameters_xgb)\n",
    "\n",
    "bestPrametersDataFrame = pd.DataFrame({\n",
    "                                        \"ParameterName\" : list(bestParameters_xgb.keys()),\n",
    "                                        \"OptimalParameter\" : list(bestParameters_xgb.values()),\n",
    "                                        \"R2\": [bestR2Score_xgb] * len(bestParameters_xgb)\n",
    "                                        });\n",
    "\n",
    "bestPrametersDataFrame.to_excel(RESULT_DIR + f\"/General_Best_Parameters.xlsx\", index=False);\n",
    "\n",
    "print(f\"\\n============<DONE WITH GRID SEARCHING>============\");\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # MODEL TRAINING (HistGradientBoostingRegressor)\n",
    "#     # ------------------------------------------------------------\n",
    "\n",
    "# model = HistGradientBoostingRegressor(**bestParameters_xgb,\n",
    "#                                         random_state=42);\n",
    "\n",
    "# # Define scoring metrics\n",
    "# scorers = {\n",
    "#     'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "#     'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#     'MedAE': make_scorer(median_absolute_error, greater_is_better=False),\n",
    "#     'R2': make_scorer(r2_score)\n",
    "# }\n",
    "\n",
    "# # Cross-validation evaluation\n",
    "# cv_results = {}\n",
    "# for name, scorer in scorers.items():\n",
    "#     scores = cross_val_score(model, X, y, cv=kf, scoring=scorer)\n",
    "#     # Flip sign for error metrics\n",
    "#     if name in ['MSE', 'MAE', 'MedAE']:\n",
    "#         scores = -scores\n",
    "#     cv_results[name] = (round(scores.mean(), 2), round(scores.std(), 2))\n",
    "\n",
    "# # Fit final model for reporting + predictions\n",
    "# model.fit(X, y)\n",
    "# y_pred_train = model.predict(X)\n",
    "\n",
    "# # Training set evaluation\n",
    "# train_results = {\n",
    "#     'MSE': (round(mean_squared_error(y, y_pred_train), 2), 0),\n",
    "#     'MAE': (round(mean_absolute_error(y, y_pred_train), 2), 0),\n",
    "#     'MedAE': (round(median_absolute_error(y, y_pred_train), 2), 0),\n",
    "#     'R2': (round(r2_score(y, y_pred_train), 2), 0)\n",
    "# }\n",
    "\n",
    "# # Save evaluation report\n",
    "# eval_report = pd.DataFrame({\n",
    "#     'Metric': list(scorers.keys()),\n",
    "#     'Cross_Validation': [cv_results[m][0] for m in scorers.keys()],\n",
    "#     'CV_SD': [cv_results[m][1] for m in scorers.keys()],\n",
    "#     'Train': [train_results[m][0] for m in scorers.keys()],\n",
    "#     'Train_SD': [train_results[m][1] for m in scorers.keys()]\n",
    "# })\n",
    "# eval_report.to_csv(f'{REPORT_DIR}Evaluation_Report_General.csv', index=False)\n",
    "# print(f\"\\n--- Model Evaluation (General) ---\\n{eval_report}\\n\")\n",
    "\n",
    "# # Save trained model\n",
    "# with open(f'{RESULT_DIR}histgb_model_General.pkl', 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # FEATURE IMPORTANCE\n",
    "# # Computed using model's built-in method or permutation fallback.\n",
    "# # ------------------------------------------------------------\n",
    "# feature_names = ['Coumarin Dose', 'Time', 'Cancer Type']\n",
    "\n",
    "# try:\n",
    "#     importance = model.feature_importances_\n",
    "# except AttributeError:\n",
    "#     from sklearn.inspection import permutation_importance\n",
    "#     result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "#     importance = result.importances_mean\n",
    "\n",
    "# # Rank features\n",
    "# sorted_idx = np.argsort(importance)\n",
    "# sorted_features = [feature_names[i] for i in sorted_idx]\n",
    "# sorted_importance = importance[sorted_idx]\n",
    "\n",
    "# # Save importance table\n",
    "# importance_df = pd.DataFrame({\n",
    "#     'Feature': sorted_features,\n",
    "#     'Importance': sorted_importance\n",
    "# })\n",
    "# importance_df.to_csv(\n",
    "#     f'{REPORT_DIR}Feature_Importance_General_HistGradientBoostingRegressor.csv',\n",
    "#     index=False\n",
    "# )\n",
    "\n",
    "# # Save TIFF plot\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.barh(sorted_features, sorted_importance, color='black')\n",
    "# plt.xlabel(\"Importance\")\n",
    "# plt.title(f\"Feature Importance – General – HistGradientBoostingRegressor\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\n",
    "#     f'{REPORT_DIR}Feature_Importance_General_HistGradientBoostingRegressor.tiff',\n",
    "#     format='tiff', dpi=300\n",
    "# )\n",
    "# plt.close()\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # OPTIMAL DOSE/TIME PREDICTION FOR TARGET VIABILITY 50\n",
    "# # Scans a dose-time grid to find setting closest to viability=50.\n",
    "# # ------------------------------------------------------------\n",
    "# def predict_viability(cancer_code):\n",
    "#     predictions = []\n",
    "#     for dose in np.linspace(0, 400, 50):\n",
    "#         for time in allowed_times:\n",
    "#             viability = model.predict([[dose, time, cancer_code]])[0]\n",
    "#             predictions.append((dose, time, viability))\n",
    "#     df_pred = pd.DataFrame(predictions, columns=['Dose', 'Time', 'Viability'])\n",
    "#     df_pred['AbsError'] = abs(df_pred['Viability'] - 50)\n",
    "#     return df_pred.loc[df_pred['AbsError'].idxmin()]\n",
    "\n",
    "# # Generate predictions for each reliable cancer type\n",
    "# results = []\n",
    "# for cancer_code in tqdm(reliable_cancers, desc=f\"Predicting {coumarin}\"):\n",
    "#     cancer_name = CancerType_Encoder.inverse_transform([cancer_code])[0]\n",
    "#     count = cancer_counts.get(cancer_code, 0)\n",
    "#     best = predict_viability(cancer_code)\n",
    "#     results.append({\n",
    "#         'Cancer Type': cancer_name,\n",
    "#         'Best Dose': best['Dose'],\n",
    "#         'Best Time': best['Time'],\n",
    "#         'Predicted Viability': best['Viability'],\n",
    "#         'Sample Count': count,\n",
    "#         'Reliability': 'Reliable'\n",
    "#     })\n",
    "\n",
    "# # Save final prediction summary\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'{REPORT_DIR}Prediction_Report_{coumarin}.csv', index=False)\n",
    "# print(f\"Prediction report for {coumarin} saved successfully.\\n\")\n",
    "\n",
    "# print(\"\\n✅ All Coumarin types processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2ff40",
   "metadata": {},
   "source": [
    "## GradientBoosting (SEPERATE COUMARINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffaa051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error,\n",
    "    median_absolute_error, r2_score, make_scorer\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# Paths for dataset, trained models, and evaluation reports.\n",
    "# ============================================================\n",
    "DATA_PATH = 'Total_Data.csv'\n",
    "RESULT_DIR = 'GridSearchingResults/Models/GradientBoostingOnSeperateCoumarins/'\n",
    "REPORT_DIR = 'GridSearchingReports/Reports/GradientBoostingOnSeperateCoumarins/'\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD & PREPARE DATA\n",
    "# Includes: loading dataset, column selection, cleaning, label encoding\n",
    "# ============================================================\n",
    "data = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
    "required_columns = ['Coumarin Type', 'Cancer Type', 'Coumarin Dose', 'Time', 'Viability']\n",
    "data = data[required_columns].dropna()\n",
    "\n",
    "# Convert dose to numeric and restrict doses ≤ 400\n",
    "data['Coumarin Dose'] = pd.to_numeric(data['Coumarin Dose'], errors='coerce')\n",
    "data = data[data['Coumarin Dose'] <= 400]\n",
    "\n",
    "# Encode cancer types for ML models\n",
    "CANCER_ENCODER = LabelEncoder()\n",
    "data['Cancer Type'] = CANCER_ENCODER.fit_transform(data['Cancer Type'])\n",
    "\n",
    "# ============================================================\n",
    "# PROCESS EACH COUMARIN TYPE\n",
    "# Each coumarin is processed independently to produce:\n",
    "# - Reliability filtering via Gaussian Mixture Model\n",
    "# - Model training using HistGradientBoostingRegressor\n",
    "# - Full cross-validation metrics\n",
    "# - Feature importance (CSV + TIFF plot)\n",
    "# - Optimal dose/time predictions for viability≈50\n",
    "# ============================================================\n",
    "for coumarin in data['Coumarin Type'].unique():\n",
    "    print(f\"\\n=== Processing {coumarin} ===\")\n",
    "\n",
    "    if coumarin in ['Auraptene', 'Esculetin'] :\n",
    "        continue\n",
    "\n",
    "    # Extract coumarin-specific subset\n",
    "    coumarin_data = data[data['Coumarin Type'] == coumarin].copy()\n",
    "\n",
    "    # Some compounds have an additional 96h measurement\n",
    "    allowed_times = [24, 48, 72, 96] if coumarin.lower() == 'auraptene' else [24, 48, 72]\n",
    "    coumarin_data = coumarin_data[coumarin_data['Time'].isin(allowed_times)]\n",
    "\n",
    "    if coumarin_data.empty:\n",
    "        print(f\"⚠️ No valid time points for {coumarin}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # RELIABILITY FILTERING USING GMM\n",
    "    # Identifies cancer types with sufficient sample size.\n",
    "    # ------------------------------------------------------------\n",
    "    cancer_counts = coumarin_data['Cancer Type'].value_counts().to_dict()\n",
    "    count_df = pd.DataFrame(list(cancer_counts.items()), columns=['Cancer Type', 'Sample Count'])\n",
    "\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(count_df[['Sample Count']])\n",
    "\n",
    "    # Threshold = midpoint between two mixture components\n",
    "    threshold = np.mean(gmm.means_.flatten())\n",
    "\n",
    "    count_df['Reliability'] = count_df['Sample Count'].apply(\n",
    "        lambda x: 'Reliable' if x >= threshold else 'Unreliable'\n",
    "    )\n",
    "\n",
    "    reliable_cancers = count_df[count_df['Reliability'] == 'Reliable']['Cancer Type'].tolist()\n",
    "    reliable_data = coumarin_data[coumarin_data['Cancer Type'].isin(reliable_cancers)].copy()\n",
    "\n",
    "    print(f\"Reliability threshold for {coumarin}: {threshold:.2f}\")\n",
    "    print(f\"Reliable cancer types retained: {len(reliable_cancers)} of {len(cancer_counts)}\")\n",
    "\n",
    "    if reliable_data.empty:\n",
    "        print(f\"⚠️ No reliable data for {coumarin}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    #-------------------------------------------------------------\n",
    "    # BEGIN GRID SEARCHING FOR OPTIMAL R2\n",
    "    #-------------------------------------------------------------\n",
    "    \n",
    "    gradientBoosting_paramGrid = {\n",
    "        'n_estimators': [50, 150, 300],\n",
    "        'learning_rate': [0.01 , 0.1, 0.2],\n",
    "        'max_depth': [2, 3, 6],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.7, 0.9, 1.0],\n",
    "        'max_features': [None, 'sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    gridSearcher = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42),\n",
    "                                param_grid=gradientBoosting_paramGrid,\n",
    "                                scoring='r2',\n",
    "                                cv=kf,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=1)\n",
    "    \n",
    "    X = reliable_data[['Coumarin Dose', 'Time', 'Cancer Type']]\n",
    "    y = reliable_data['Viability']\n",
    "    \n",
    "    print(f\"\\n===<GRID SEARCHING FOR {coumarin.upper()}>===\")\n",
    "    gridSearcher.fit(X,y);\n",
    "    \n",
    "    BEST_PARAMETERS = gridSearcher.best_params_;\n",
    "    BEST_PARAMETERS[\"R2\"] = gridSearcher.best_score_;\n",
    "    df = pd.DataFrame([BEST_PARAMETERS]);\n",
    "    print(\"BEST PARAMETERS: \", df)\n",
    "\n",
    "    df.to_csv(RESULT_DIR + f\"{coumarin}_gba_BEST_PARAMETERS.csv\",index=False)\n",
    "    BEST_PARAMETERS.pop('R2')\n",
    "\n",
    "    print(\"===<DONE WITH GRID SEARCHING>===\")\n",
    "    # ------------------------------------------------------------\n",
    "    # MODEL TRAINING (GradientBoostingRegressor)\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    model = GradientBoostingRegressor(**BEST_PARAMETERS, random_state=42)\n",
    "\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scorers = {\n",
    "        'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "        'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "        'MedAE': make_scorer(median_absolute_error, greater_is_better=False),\n",
    "        'R2': make_scorer(r2_score)\n",
    "    }\n",
    "\n",
    "    # Cross-validation evaluation\n",
    "    cv_results = {}\n",
    "    for name, scorer in scorers.items():\n",
    "        scores = cross_val_score(model, X, y, cv=kf, scoring=scorer)\n",
    "        # Flip sign for error metrics\n",
    "        if name in ['MSE', 'MAE', 'MedAE']:\n",
    "            scores = -scores\n",
    "        cv_results[name] = (round(scores.mean(), 2), round(scores.std(), 2))\n",
    "\n",
    "    # Fit final model for reporting + predictions\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    cv_results = pd.DataFrame(zip(cv_results));\n",
    "\n",
    "    print(cv_results);\n",
    "\n",
    "    cv_results.to_csv(f'{REPORT_DIR}Evaluation_Report_{coumarin}.csv', index=False)\n",
    "    print(f\"\\n--- Model Evaluation ({coumarin}) ---\\n{cv_results}\\n\")\n",
    "\n",
    "    # Save trained model\n",
    "    with open(f'{RESULT_DIR}gba_model_{coumarin}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # FEATURE IMPORTANCE\n",
    "    # Computed using model's built-in method or permutation fallback.\n",
    "    # ------------------------------------------------------------\n",
    "    feature_names = ['Coumarin Dose', 'Time', 'Cancer Type']\n",
    "\n",
    "    try:\n",
    "        importance = model.feature_importances_\n",
    "    except AttributeError:\n",
    "        from sklearn.inspection import permutation_importance\n",
    "        result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "        importance = result.importances_mean\n",
    "\n",
    "    # Rank features\n",
    "    sorted_idx = np.argsort(importance)\n",
    "    sorted_features = [feature_names[i] for i in sorted_idx]\n",
    "    sorted_importance = importance[sorted_idx]\n",
    "\n",
    "    # Save importance table\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': sorted_features,\n",
    "        'Importance': sorted_importance\n",
    "    })\n",
    "    importance_df.to_csv(\n",
    "        f'{REPORT_DIR}Feature_Importance_{coumarin}_gbaOnSeperateCoumarins.csv',\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Save TIFF plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(sorted_features, sorted_importance, color='black')\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(f\"Feature Importance – {coumarin} – gbaOnSeperateCoumarins\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f'{REPORT_DIR}Feature_Importance_{coumarin}_gbaOnSeperateCoumarins.tiff',\n",
    "        format='tiff', dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # OPTIMAL DOSE/TIME PREDICTION FOR TARGET VIABILITY 50\n",
    "    # Scans a dose-time grid to find setting closest to viability=50.\n",
    "    # ------------------------------------------------------------\n",
    "    def predict_viability(cancer_code):\n",
    "        predictions = []\n",
    "        for dose in np.linspace(0, 400, 50):\n",
    "            for time in allowed_times:\n",
    "                viability = model.predict([[dose, time, cancer_code]])[0]\n",
    "                predictions.append((dose, time, viability))\n",
    "        df_pred = pd.DataFrame(predictions, columns=['Dose', 'Time', 'Viability'])\n",
    "        df_pred['AbsError'] = abs(df_pred['Viability'] - 50)\n",
    "        return df_pred.loc[df_pred['AbsError'].idxmin()]\n",
    "\n",
    "    cancers = data['Cancer Type'].unique();\n",
    "\n",
    "    # Generate predictions for each reliable cancer type\n",
    "    results = []\n",
    "    for cancer_code in tqdm(cancers, desc=f\"Predicting {coumarin}\"):\n",
    "        cancer_name = CANCER_ENCODER.inverse_transform([cancer_code])[0]\n",
    "        # count = cancer_counts.get(cancer_code, 0)\n",
    "        best = predict_viability(cancer_code)\n",
    "        results.append({\n",
    "            'Cancer Type': cancer_name,\n",
    "            'Best Dose': best['Dose'],\n",
    "            'Best Time': best['Time'],\n",
    "            'Predicted Viability': best['Viability'],\n",
    "            # 'Sample Count': count,\n",
    "            'Reliability': 'Reliable'\n",
    "        })\n",
    "\n",
    "    # Save final prediction summary\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'{REPORT_DIR}Prediction_Report_{coumarin}.csv', index=False)\n",
    "    print(f\"Prediction report for {coumarin} saved successfully.\\n\")\n",
    "\n",
    "print(\"\\n✅ All Coumarin types processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce15fa2",
   "metadata": {},
   "source": [
    "## GradientBoositngRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error,\n",
    "    median_absolute_error, r2_score, make_scorer\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# Paths for dataset, trained models, and evaluation reports.\n",
    "# ============================================================\n",
    "DATA_PATH = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/Total_Data.csv'\n",
    "RESULT_DIR = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/GridSearchingResults/Models/GradientBoosting'\n",
    "REPORT_DIR = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/GridSearchingResults/Reports/GradientBoosting'\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD & PREPARE DATA\n",
    "# Includes: loading dataset, column selection, cleaning, label encoding\n",
    "# ============================================================\n",
    "data = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
    "required_columns = ['Coumarin Type', 'Cancer Type', 'Coumarin Dose', 'Time', 'Viability']\n",
    "data = data[required_columns].dropna()\n",
    "\n",
    "# Convert dose to numeric and restrict doses ≤ 400\n",
    "data['Coumarin Dose'] = pd.to_numeric(data['Coumarin Dose'], errors='coerce')\n",
    "data = data[data['Coumarin Dose'] <= 400]\n",
    "\n",
    "# Encode cancer types for ML models\n",
    "cancerType_Encoder = LabelEncoder();\n",
    "coumarin_Encoder = LabelEncoder();\n",
    "data['Cancer Type'] = cancerType_Encoder.fit_transform(data['Cancer Type'])\n",
    "data['Coumarin Type'] = coumarin_Encoder.fit_transform(data['Coumarin Type'])\n",
    "\n",
    "# Some compounds have an additional 96h measurement\n",
    "allowed_times = [24, 48, 72, 96];\n",
    "auraptene_allowed_time = [24, 48, 72];\n",
    "data = data[\n",
    "    ((data['Coumarin Type'] != \"Auraptene\") & (data['Time'].isin(allowed_times))) |\n",
    "    ((data['Coumarin Type'] == \"Auraptene\") & (data['Time'].isin(auraptene_allowed_time)))\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# GRID SEARCHING FOR OPTIMAL HYPER PARAMETERS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n=====================================<GRID SEARCHING>=====================================\")\n",
    "\n",
    "\n",
    "gradientBoosting_paramGrid = {\n",
    "    'n_estimators': [50, 150, 300],\n",
    "    'learning_rate': [0.01 , 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4, 6],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X = data[['Coumarin Type', 'Coumarin Dose', 'Time', 'Cancer Type']]\n",
    "y = data['Viability']\n",
    "\n",
    "\n",
    "\n",
    "GradientBoosting_gridSearcher = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42),\n",
    "                                            param_grid=gradientBoosting_paramGrid,\n",
    "                                            scoring='r2',\n",
    "                                            cv=kf,\n",
    "                                            n_jobs=-1,\n",
    "                                            verbose=1\n",
    "                                            )\n",
    "\n",
    "GradientBoosting_gridSearcher.fit(X,y)\n",
    "bestParameters_GradientBoosting = GradientBoosting_gridSearcher.best_params_;\n",
    "bestR2Score_GradientBoosting = GradientBoosting_gridSearcher.best_score_;\n",
    "\n",
    "print(f\"\\nBEST R2 SCORE : {bestR2Score_GradientBoosting}\")\n",
    "print(bestParameters_GradientBoosting)\n",
    "#{'learning_rate': 0.1, 'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300, 'subsample': 1.0}\n",
    "# BEST R2 SCORE : 0.6540099951366823\n",
    "bestPrametersDataFrame = pd.DataFrame({\n",
    "                                        \"ParameterName\" : list(bestParameters_GradientBoosting.keys()),\n",
    "                                        \"OptimalParameter\" : list(bestParameters_GradientBoosting.values()),\n",
    "                                        \"R2\": [bestR2Score_GradientBoosting] * len(bestParameters_GradientBoosting)\n",
    "                                        });\n",
    "\n",
    "bestPrametersDataFrame.to_excel(RESULT_DIR + f\"/General_Best_Parameters.xlsx\", index=False);\n",
    "\n",
    "print(f\"\\n============<DONE WITH GRID SEARCHING>============\");\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # MODEL TRAINING (HistGradientBoostingRegressor)\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "# model = HistGradientBoostingRegressor(**bestParameters_adaRegressor,\n",
    "#                                         random_state=42);\n",
    "\n",
    "# # Define scoring metrics\n",
    "# scorers = {\n",
    "#     'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "#     'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#     'MedAE': make_scorer(median_absolute_error, greater_is_better=False),\n",
    "#     'R2': make_scorer(r2_score)\n",
    "# }\n",
    "\n",
    "# # Cross-validation evaluation\n",
    "# cv_results = {}\n",
    "# for name, scorer in scorers.items():\n",
    "#     scores = cross_val_score(model, X, y, cv=kf, scoring=scorer)\n",
    "#     # Flip sign for error metrics\n",
    "#     if name in ['MSE', 'MAE', 'MedAE']:\n",
    "#         scores = -scores\n",
    "#     cv_results[name] = (round(scores.mean(), 2), round(scores.std(), 2))\n",
    "\n",
    "# # Fit final model for reporting + predictions\n",
    "# model.fit(X, y)\n",
    "# y_pred_train = model.predict(X)\n",
    "\n",
    "# # Training set evaluation\n",
    "# train_results = {\n",
    "#     'MSE': (round(mean_squared_error(y, y_pred_train), 2), 0),\n",
    "#     'MAE': (round(mean_absolute_error(y, y_pred_train), 2), 0),\n",
    "#     'MedAE': (round(median_absolute_error(y, y_pred_train), 2), 0),\n",
    "#     'R2': (round(r2_score(y, y_pred_train), 2), 0)\n",
    "# }\n",
    "\n",
    "# # Save evaluation report\n",
    "# eval_report = pd.DataFrame({\n",
    "#     'Metric': list(scorers.keys()),\n",
    "#     'Cross_Validation': [cv_results[m][0] for m in scorers.keys()],\n",
    "#     'CV_SD': [cv_results[m][1] for m in scorers.keys()],\n",
    "#     'Train': [train_results[m][0] for m in scorers.keys()],\n",
    "#     'Train_SD': [train_results[m][1] for m in scorers.keys()]\n",
    "# })\n",
    "# eval_report.to_csv(f'{REPORT_DIR}Evaluation_Report_General.csv', index=False)\n",
    "# print(f\"\\n--- Model Evaluation (General) ---\\n{eval_report}\\n\")\n",
    "\n",
    "# # Save trained model\n",
    "# with open(f'{RESULT_DIR}histgb_model_General.pkl', 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # FEATURE IMPORTANCE\n",
    "# # Computed using model's built-in method or permutation fallback.\n",
    "# # ------------------------------------------------------------\n",
    "# feature_names = ['Coumarin Dose', 'Time', 'Cancer Type']\n",
    "\n",
    "# try:\n",
    "#     importance = model.feature_importances_\n",
    "# except AttributeError:\n",
    "#     from sklearn.inspection import permutation_importance\n",
    "#     result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "#     importance = result.importances_mean\n",
    "\n",
    "# # Rank features\n",
    "# sorted_idx = np.argsort(importance)\n",
    "# sorted_features = [feature_names[i] for i in sorted_idx]\n",
    "# sorted_importance = importance[sorted_idx]\n",
    "\n",
    "# # Save importance table\n",
    "# importance_df = pd.DataFrame({\n",
    "#     'Feature': sorted_features,\n",
    "#     'Importance': sorted_importance\n",
    "# })\n",
    "# importance_df.to_csv(\n",
    "#     f'{REPORT_DIR}Feature_Importance_General_HistGradientBoostingRegressor.csv',\n",
    "#     index=False\n",
    "# )\n",
    "\n",
    "# # Save TIFF plot\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.barh(sorted_features, sorted_importance, color='black')\n",
    "# plt.xlabel(\"Importance\")\n",
    "# plt.title(f\"Feature Importance – General – HistGradientBoostingRegressor\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\n",
    "#     f'{REPORT_DIR}Feature_Importance_General_HistGradientBoostingRegressor.tiff',\n",
    "#     format='tiff', dpi=300\n",
    "# )\n",
    "# plt.close()\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # OPTIMAL DOSE/TIME PREDICTION FOR TARGET VIABILITY 50\n",
    "# # Scans a dose-time grid to find setting closest to viability=50.\n",
    "# # ------------------------------------------------------------\n",
    "# def predict_viability(cancer_code):\n",
    "#     predictions = []\n",
    "#     for dose in np.linspace(0, 400, 50):\n",
    "#         for time in allowed_times:\n",
    "#             viability = model.predict([[dose, time, cancer_code]])[0]\n",
    "#             predictions.append((dose, time, viability))\n",
    "#     df_pred = pd.DataFrame(predictions, columns=['Dose', 'Time', 'Viability'])\n",
    "#     df_pred['AbsError'] = abs(df_pred['Viability'] - 50)\n",
    "#     return df_pred.loc[df_pred['AbsError'].idxmin()]\n",
    "\n",
    "# # Generate predictions for each reliable cancer type\n",
    "# results = []\n",
    "# for cancer_code in tqdm(reliable_cancers, desc=f\"Predicting {coumarin}\"):\n",
    "#     cancer_name = CancerType_Encoder.inverse_transform([cancer_code])[0]\n",
    "#     count = cancer_counts.get(cancer_code, 0)\n",
    "#     best = predict_viability(cancer_code)\n",
    "#     results.append({\n",
    "#         'Cancer Type': cancer_name,\n",
    "#         'Best Dose': best['Dose'],\n",
    "#         'Best Time': best['Time'],\n",
    "#         'Predicted Viability': best['Viability'],\n",
    "#         'Sample Count': count,\n",
    "#         'Reliability': 'Reliable'\n",
    "#     })\n",
    "\n",
    "# # Save final prediction summary\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'{REPORT_DIR}Prediction_Report_{coumarin}.csv', index=False)\n",
    "# print(f\"Prediction report for {coumarin} saved successfully.\\n\")\n",
    "\n",
    "# print(\"\\n✅ All Coumarin types processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5b801",
   "metadata": {},
   "source": [
    "## HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error,\n",
    "    median_absolute_error, r2_score, make_scorer\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# Paths for dataset, trained models, and evaluation reports.\n",
    "# ============================================================\n",
    "DATA_PATH = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/Total_Data.csv'\n",
    "RESULT_DIR = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/GridSearchingResults/Models/HistGradientBoostingRegressor'\n",
    "REPORT_DIR = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/GridSearchingResults/Reports/HistGradientBoostingRegressor'\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD & PREPARE DATA\n",
    "# Includes: loading dataset, column selection, cleaning, label encoding\n",
    "# ============================================================\n",
    "data = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
    "required_columns = ['Coumarin Type', 'Cancer Type', 'Coumarin Dose', 'Time', 'Viability']\n",
    "data = data[required_columns].dropna()\n",
    "\n",
    "# Convert dose to numeric and restrict doses ≤ 400\n",
    "data['Coumarin Dose'] = pd.to_numeric(data['Coumarin Dose'], errors='coerce')\n",
    "data = data[data['Coumarin Dose'] <= 400]\n",
    "\n",
    "# Encode cancer types for ML models\n",
    "cancerType_Encoder = LabelEncoder();\n",
    "coumarin_Encoder = LabelEncoder();\n",
    "data['Cancer Type'] = cancerType_Encoder.fit_transform(data['Cancer Type'])\n",
    "data['Coumarin Type'] = coumarin_Encoder.fit_transform(data['Coumarin Type'])\n",
    "\n",
    "# Some compounds have an additional 96h measurement\n",
    "allowed_times = [24, 48, 72, 96];\n",
    "auraptene_allowed_time = [24, 48, 72];\n",
    "data = data[\n",
    "    ((data['Coumarin Type'] != \"Auraptene\") & (data['Time'].isin(allowed_times))) |\n",
    "    ((data['Coumarin Type'] == \"Auraptene\") & (data['Time'].isin(auraptene_allowed_time)))\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# GRID SEARCHING FOR OPTIMAL HYPER PARAMETERS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n=====================================<GRID SEARCHING>=====================================\")\n",
    "\n",
    "# {'early_stopping': True, 'l2_regularization': 1.5, 'learning_rate': 0.1, 'max_bins': 128, 'max_depth': None,\n",
    "#  'max_iter': 100, 'max_leaf_nodes': 31, 'min_samples_leaf': 10, 'n_iter_no_change': 20, 'validation_fraction': 0.1}\n",
    "# BEST R2 SCORE ON : 0.6206090802796808\n",
    "histGradientRegressor_paramGrid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_iter': [100, 200],\n",
    "    'max_leaf_nodes': [31, 63, 255],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [10, 20, 50],\n",
    "    'l2_regularization': [0.0, 0.1, 1.5],\n",
    "    'max_bins': [128, 255],\n",
    "    'early_stopping': [True],\n",
    "    'validation_fraction': [0.1],\n",
    "    'n_iter_no_change': [10, 20]\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X = data[['Coumarin Type', 'Coumarin Dose', 'Time', 'Cancer Type']]\n",
    "y = data['Viability']\n",
    "\n",
    "histGradient_gridSearcher = GridSearchCV(estimator=HistGradientBoostingRegressor(random_state=42),\n",
    "                                            param_grid=histGradientRegressor_paramGrid,\n",
    "                                            scoring='r2',\n",
    "                                            cv=kf,\n",
    "                                            n_jobs=-1,\n",
    "                                            verbose=1\n",
    "                                            )\n",
    "\n",
    "histGradient_gridSearcher.fit(X,y)\n",
    "bestParameters_HistGradientBoostingRegressor = histGradient_gridSearcher.best_params_;\n",
    "bestR2Score_HistGradientBoostingRegressor = histGradient_gridSearcher.best_score_;\n",
    "\n",
    "print(f\"\\nBEST R2 SCORE ON : {bestR2Score_HistGradientBoostingRegressor}\")\n",
    "print(bestParameters_HistGradientBoostingRegressor)\n",
    "\n",
    "bestPrametersDataFrame = pd.DataFrame({\n",
    "                                        \"ParameterName\" : list(bestParameters_HistGradientBoostingRegressor.keys()),\n",
    "                                        \"OptimalParameter\" : list(bestParameters_HistGradientBoostingRegressor.values()),\n",
    "                                        \"R2\": [bestR2Score_HistGradientBoostingRegressor] * len(bestParameters_HistGradientBoostingRegressor)\n",
    "                                        });\n",
    "\n",
    "bestPrametersDataFrame.to_excel(RESULT_DIR + f\"/General_Best_Parameters.xlsx\", index=False);\n",
    "\n",
    "print(f\"\\n============<DONE WITH GRID SEARCHING>============\");\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # MODEL TRAINING (HistGradientBoostingRegressor)\n",
    "#     # ------------------------------------------------------------\n",
    "\n",
    "# model = HistGradientBoostingRegressor(**bestParameters_HistGradientBoostingRegressor,\n",
    "#                                         random_state=42);\n",
    "\n",
    "# # Define scoring metrics\n",
    "# scorers = {\n",
    "#     'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "#     'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#     'MedAE': make_scorer(median_absolute_error, greater_is_better=False),\n",
    "#     'R2': make_scorer(r2_score)\n",
    "# }\n",
    "\n",
    "# # Cross-validation evaluation\n",
    "# cv_results = {}\n",
    "# for name, scorer in scorers.items():\n",
    "#     scores = cross_val_score(model, X, y, cv=kf, scoring=scorer)\n",
    "#     # Flip sign for error metrics\n",
    "#     if name in ['MSE', 'MAE', 'MedAE']:\n",
    "#         scores = -scores\n",
    "#     cv_results[name] = (round(scores.mean(), 2), round(scores.std(), 2))\n",
    "\n",
    "# # Fit final model for reporting + predictions\n",
    "# model.fit(X, y)\n",
    "# y_pred_train = model.predict(X)\n",
    "\n",
    "# # Training set evaluation\n",
    "# train_results = {\n",
    "#     'MSE': (round(mean_squared_error(y, y_pred_train), 2), 0),\n",
    "#     'MAE': (round(mean_absolute_error(y, y_pred_train), 2), 0),\n",
    "#     'MedAE': (round(median_absolute_error(y, y_pred_train), 2), 0),\n",
    "#     'R2': (round(r2_score(y, y_pred_train), 2), 0)\n",
    "# }\n",
    "\n",
    "# # Save evaluation report\n",
    "# eval_report = pd.DataFrame({\n",
    "#     'Metric': list(scorers.keys()),\n",
    "#     'Cross_Validation': [cv_results[m][0] for m in scorers.keys()],\n",
    "#     'CV_SD': [cv_results[m][1] for m in scorers.keys()],\n",
    "#     'Train': [train_results[m][0] for m in scorers.keys()],\n",
    "#     'Train_SD': [train_results[m][1] for m in scorers.keys()]\n",
    "# })\n",
    "# eval_report.to_csv(f'{REPORT_DIR}Evaluation_Report_General.csv', index=False)\n",
    "# print(f\"\\n--- Model Evaluation (General) ---\\n{eval_report}\\n\")\n",
    "\n",
    "# # Save trained model\n",
    "# with open(f'{RESULT_DIR}histgb_model_General.pkl', 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # FEATURE IMPORTANCE\n",
    "# # Computed using model's built-in method or permutation fallback.\n",
    "# # ------------------------------------------------------------\n",
    "# feature_names = ['Coumarin Dose', 'Time', 'Cancer Type']\n",
    "\n",
    "# try:\n",
    "#     importance = model.feature_importances_\n",
    "# except AttributeError:\n",
    "#     from sklearn.inspection import permutation_importance\n",
    "#     result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "#     importance = result.importances_mean\n",
    "\n",
    "# # Rank features\n",
    "# sorted_idx = np.argsort(importance)\n",
    "# sorted_features = [feature_names[i] for i in sorted_idx]\n",
    "# sorted_importance = importance[sorted_idx]\n",
    "\n",
    "# # Save importance table\n",
    "# importance_df = pd.DataFrame({\n",
    "#     'Feature': sorted_features,\n",
    "#     'Importance': sorted_importance\n",
    "# })\n",
    "# importance_df.to_csv(\n",
    "#     f'{REPORT_DIR}Feature_Importance_General_HistGradientBoostingRegressor.csv',\n",
    "#     index=False\n",
    "# )\n",
    "\n",
    "# # Save TIFF plot\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.barh(sorted_features, sorted_importance, color='black')\n",
    "# plt.xlabel(\"Importance\")\n",
    "# plt.title(f\"Feature Importance – General – HistGradientBoostingRegressor\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\n",
    "#     f'{REPORT_DIR}Feature_Importance_General_HistGradientBoostingRegressor.tiff',\n",
    "#     format='tiff', dpi=300\n",
    "# )\n",
    "# plt.close()\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # OPTIMAL DOSE/TIME PREDICTION FOR TARGET VIABILITY 50\n",
    "# # Scans a dose-time grid to find setting closest to viability=50.\n",
    "# # ------------------------------------------------------------\n",
    "# def predict_viability(cancer_code):\n",
    "#     predictions = []\n",
    "#     for dose in np.linspace(0, 400, 50):\n",
    "#         for time in allowed_times:\n",
    "#             viability = model.predict([[dose, time, cancer_code]])[0]\n",
    "#             predictions.append((dose, time, viability))\n",
    "#     df_pred = pd.DataFrame(predictions, columns=['Dose', 'Time', 'Viability'])\n",
    "#     df_pred['AbsError'] = abs(df_pred['Viability'] - 50)\n",
    "#     return df_pred.loc[df_pred['AbsError'].idxmin()]\n",
    "\n",
    "# # Generate predictions for each reliable cancer type\n",
    "# results = []\n",
    "# for cancer_code in tqdm(reliable_cancers, desc=f\"Predicting {coumarin}\"):\n",
    "#     cancer_name = CancerType_Encoder.inverse_transform([cancer_code])[0]\n",
    "#     count = cancer_counts.get(cancer_code, 0)\n",
    "#     best = predict_viability(cancer_code)\n",
    "#     results.append({\n",
    "#         'Cancer Type': cancer_name,\n",
    "#         'Best Dose': best['Dose'],\n",
    "#         'Best Time': best['Time'],\n",
    "#         'Predicted Viability': best['Viability'],\n",
    "#         'Sample Count': count,\n",
    "#         'Reliability': 'Reliable'\n",
    "#     })\n",
    "\n",
    "# # Save final prediction summary\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'{REPORT_DIR}Prediction_Report_{coumarin}.csv', index=False)\n",
    "# print(f\"Prediction report for {coumarin} saved successfully.\\n\")\n",
    "\n",
    "# print(\"\\n✅ All Coumarin types processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c8faa2",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e8fbb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scopoletin data excluded. Remaining coumarins: ['Auraptene', 'Esculetin', 'Galbanic Acid', 'Umbelliprenin']\n",
      "\n",
      "=== Processing ===\n",
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m     98\u001b[39m kf = KFold(n_splits=\u001b[32m5\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m);\n\u001b[32m    100\u001b[39m gridSearcher_ForestRegressor = GridSearchCV(estimator=RandomForestRegressor(random_state=\u001b[32m42\u001b[39m),\n\u001b[32m    101\u001b[39m                                             param_grid=param_grid_ForestRegressor,\n\u001b[32m    102\u001b[39m                                             scoring=\u001b[33m'\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    103\u001b[39m                                             cv=kf,\n\u001b[32m    104\u001b[39m                                             n_jobs=-\u001b[32m1\u001b[39m,\n\u001b[32m    105\u001b[39m                                             verbose=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[43mgridSearcher_ForestRegressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m bestParameters_ForestRegressor = gridSearcher_ForestRegressor.best_params_;\n\u001b[32m    110\u001b[39m bestR2Score_ForestRegressor = gridSearcher_ForestRegressor.best_score_;\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sklearn/model_selection/_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch-env/lib/python3.13/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score, make_scorer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "DATA_PATH = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/Total_Data.csv'\n",
    "RESULT_DIR = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/GridSearchingResults/Models/RandomForestRegressor'\n",
    "REPORT_DIR = '/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/GridSearchingResults/Reports/RandomForestRegressor'\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD & CLEAN DATA\n",
    "# ============================================================\n",
    "data = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
    "required_columns = ['Coumarin Type', 'Cancer Type', 'Coumarin Dose', 'Time', 'Viability']\n",
    "data = data[required_columns].dropna()\n",
    "\n",
    "data['Coumarin Dose'] = pd.to_numeric(data['Coumarin Dose'], errors='coerce')\n",
    "data = data[data['Coumarin Dose'] <= 400]\n",
    "\n",
    "# Exclude Scopoletin\n",
    "data = data[data['Coumarin Type'].str.lower() != 'scopoletin']\n",
    "print(f\"✅ Scopoletin data excluded. Remaining coumarins: {data['Coumarin Type'].unique().tolist()}\")\n",
    "\n",
    "# ============================================================\n",
    "# MAIN LOOP: PROCESS EACH COUMARIN TYPE\n",
    "# ============================================================\n",
    "print(f\"\\n=== Processing ===\")\n",
    "\n",
    "# Time filter\n",
    "allowed_times = [24, 48, 72, 96];\n",
    "auraptene_allowed_time = [24, 48, 72];\n",
    "data = data[\n",
    "    ((data['Coumarin Type'] != \"Auraptene\") & (data['Time'].isin(allowed_times))) |\n",
    "    ((data['Coumarin Type'] == \"Auraptene\") & (data['Time'].isin(auraptene_allowed_time)))\n",
    "]\n",
    "# Encode cancer types for this coumarin\n",
    "cancerType_Encoder = LabelEncoder();\n",
    "coumarin_Encoder = LabelEncoder();\n",
    "data['Cancer Type'] = cancerType_Encoder.fit_transform(data['Cancer Type'])\n",
    "data['Coumarin Type'] = coumarin_Encoder.fit_transform(data['Coumarin Type'])\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # RELIABILITY FILTERING\n",
    "# # ------------------------------------------------------------\n",
    "# cancer_counts = coumarin_data['Cancer Type'].value_counts().to_dict()\n",
    "# count_df = pd.DataFrame(list(cancer_counts.items()), columns=['Cancer Type', 'Sample Count'])\n",
    "\n",
    "# gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "# gmm.fit(count_df[['Sample Count']])\n",
    "# threshold = np.mean(gmm.means_.flatten())\n",
    "\n",
    "# count_df['Reliability'] = count_df['Sample Count'].apply(lambda x: 'Reliable' if x >= threshold else 'Unreliable')\n",
    "# count_df.to_csv(f'{REPORT_DIR}Reliability_Threshold_{coumarin}.csv', index=False)\n",
    "\n",
    "# reliable_cancers = count_df[count_df['Reliability'] == 'Reliable']['Cancer Type'].tolist()\n",
    "# reliable_data = coumarin_data[coumarin_data['Cancer Type'].isin(reliable_cancers)].copy()\n",
    "\n",
    "# print(f\"Reliability threshold for {coumarin}: {threshold:.2f}\")\n",
    "# print(f\"Reliable cancer types retained: {len(reliable_cancers)} of {len(cancer_counts)}\")\n",
    "\n",
    "# if reliable_data.empty:\n",
    "#     print(f\"⚠️ No reliable data for {coumarin}. Skipping...\")\n",
    "#     continue\n",
    "\n",
    "\n",
    "#------<BEGIN GRIDSEARCHING FOR OPTIMAL R2>----------------\n",
    "\n",
    "# {'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'max_samples': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
    "# BEST R2 SCORE : 0.5946859558299303\n",
    "X = data[['Coumarin Type', 'Coumarin Dose', 'Time', 'Cancer Type']]\n",
    "y = data['Viability']\n",
    "\n",
    "param_grid_ForestRegressor = {\n",
    "    'n_estimators': [100, 300, 600],\n",
    "    'max_depth': [None, 10, 20, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 6],\n",
    "    'max_features': ['auto', 'sqrt', 0.3, 0.7],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_samples': [None, 0.7, 0.9],\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42);\n",
    "\n",
    "gridSearcher_ForestRegressor = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
    "                                            param_grid=param_grid_ForestRegressor,\n",
    "                                            scoring='r2',\n",
    "                                            cv=kf,\n",
    "                                            n_jobs=-1,\n",
    "                                            verbose=1)\n",
    "\n",
    "\n",
    "gridSearcher_ForestRegressor.fit(X,y)\n",
    "bestParameters_ForestRegressor = gridSearcher_ForestRegressor.best_params_;\n",
    "bestR2Score_ForestRegressor = gridSearcher_ForestRegressor.best_score_;\n",
    "\n",
    "print(f\"\\nBEST R2 SCORE : {bestR2Score_ForestRegressor}\")\n",
    "\n",
    "print(bestParameters_ForestRegressor);\n",
    "\n",
    "bestPrametersDataFrame = pd.DataFrame({\n",
    "                                        \"ParameterName\" : list(bestParameters_ForestRegressor.keys()),\n",
    "                                        \"OptimalParameter\" : list(bestParameters_ForestRegressor.values()),\n",
    "                                        \"R2\": [bestR2Score_ForestRegressor] * len(bestParameters_ForestRegressor)\n",
    "                                        });\n",
    "\n",
    "bestPrametersDataFrame.to_excel(RESULT_DIR + f\"/best_parameters.xlsx\", index=False);\n",
    "\n",
    "print(f\"============<DONE WITH GRID SEARCHING>============\");\n",
    "\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # MODEL TRAINING (RandomForest)\n",
    "#     # ------------------------------------------------------------\n",
    "\n",
    "#     model = RandomForestRegressor(\n",
    "#         n_estimators=200,\n",
    "#         max_depth=20,\n",
    "#         min_samples_split=5,\n",
    "#         min_samples_leaf=2,\n",
    "#         max_features='log2',\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     scorers = {\n",
    "#         'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "#         'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#         'MedAE': make_scorer(median_absolute_error, greater_is_better=False),\n",
    "#         'R2': make_scorer(r2_score)\n",
    "#     }\n",
    "\n",
    "#     cv_results = {}\n",
    "#     for name, scorer in scorers.items():\n",
    "#         scores = cross_val_score(model, X, y, cv=kf, scoring=scorer)\n",
    "#         if name in ['MSE', 'MAE', 'MedAE']:\n",
    "#             scores = -scores\n",
    "#         cv_results[name] = (round(scores.mean(), 2), round(scores.std(), 2))\n",
    "\n",
    "#     # Fit final model\n",
    "#     model.fit(X, y)\n",
    "#     y_train_pred = model.predict(X)\n",
    "#     train_results = {\n",
    "#         'MSE': (round(mean_squared_error(y, y_train_pred), 2), 0),\n",
    "#         'MAE': (round(mean_absolute_error(y, y_train_pred), 2), 0),\n",
    "#         'MedAE': (round(median_absolute_error(y, y_train_pred), 2), 0),\n",
    "#         'R2': (round(r2_score(y, y_train_pred), 2), 0)\n",
    "#     }\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # EVALUATION REPORT\n",
    "#     # ------------------------------------------------------------\n",
    "#     eval_report = pd.DataFrame({\n",
    "#         'Metric': list(scorers.keys()),\n",
    "#         'Cross_Validation': [cv_results[m][0] for m in scorers.keys()],\n",
    "#         'CV_SD': [cv_results[m][1] for m in scorers.keys()],\n",
    "#         'Train': [train_results[m][0] for m in scorers.keys()],\n",
    "#         'Train_SD': [train_results[m][1] for m in scorers.keys()]\n",
    "#     })\n",
    "#     eval_report.to_csv(f'{REPORT_DIR}Evaluation_Report_{coumarin}.csv', index=False)\n",
    "#     print(f\"\\n--- Model Evaluation ({coumarin}) ---\\n{eval_report}\\n\")\n",
    "\n",
    "#     # Save model\n",
    "#     with open(f'{RESULT_DIR}rf_model_{coumarin}.pkl', 'wb') as f:\n",
    "#         pickle.dump(model, f)\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # FEATURE IMPORTANCE\n",
    "#     # ------------------------------------------------------------\n",
    "#     feature_names = ['Coumarin Dose', 'Time', 'Cancer Type']\n",
    "#     importance = model.feature_importances_\n",
    "\n",
    "#     sorted_idx = np.argsort(importance)\n",
    "#     sorted_features = [feature_names[i] for i in sorted_idx]\n",
    "#     sorted_importance = importance[sorted_idx]\n",
    "\n",
    "#     importance_df = pd.DataFrame({\n",
    "#         'Feature': sorted_features,\n",
    "#         'Importance': sorted_importance\n",
    "#     })\n",
    "#     importance_df.to_csv(f'{REPORT_DIR}Feature_Importance_{coumarin}_RandomForestRegressor.csv', index=False)\n",
    "\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     plt.barh(sorted_features, sorted_importance, color='black')\n",
    "#     plt.xlabel(\"Importance\")\n",
    "#     plt.title(f\"Feature Importance – {coumarin} – RandomForestRegressor\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'{REPORT_DIR}Feature_Importance_{coumarin}_RandomForestRegressor.tiff', format='tiff', dpi=300)\n",
    "#     plt.close()\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # PREDICTION REPORT\n",
    "#     # ------------------------------------------------------------\n",
    "#     def predict_viability(cancer_code):\n",
    "#         predictions = []\n",
    "#         for dose in np.linspace(0, 400, 50):\n",
    "#             for time in times:\n",
    "#                 viability = model.predict([[dose, time, cancer_code]])[0]\n",
    "#                 predictions.append((dose, time, viability))\n",
    "#         df_pred = pd.DataFrame(predictions, columns=['Dose', 'Time', 'Viability'])\n",
    "#         df_pred['AbsError'] = abs(df_pred['Viability'] - 50)\n",
    "#         return df_pred.loc[df_pred['AbsError'].idxmin()]\n",
    "\n",
    "#     results = []\n",
    "#     for cancer_code in tqdm(reliable_cancers, desc=f\"Predicting {coumarin}\"):\n",
    "#         cancer_name = label_encoder.inverse_transform([cancer_code])[0]\n",
    "#         count = cancer_counts.get(cancer_code, 0)\n",
    "#         best = predict_viability(cancer_code)\n",
    "#         results.append({\n",
    "#             'Cancer Type': cancer_name,\n",
    "#             'Best Dose': best['Dose'],\n",
    "#             'Best Time': best['Time'],\n",
    "#             'Predicted Viability': best['Viability'],\n",
    "#             'Sample Count': count,\n",
    "#             'Reliability': 'Reliable'\n",
    "#         })\n",
    "\n",
    "#     results_df = pd.DataFrame(results)\n",
    "#     results_df.to_csv(f'{REPORT_DIR}Prediction_Report_{coumarin}.csv', index=False)\n",
    "#     print(f\"\\nPrediction report for {coumarin} saved successfully.\\n\")\n",
    "\n",
    "# print(\"\\n✅ All Coumarin types processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4731f51",
   "metadata": {},
   "source": [
    "## Testing File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d03ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arashp/Programming_Files/DoseTimeOptimizations_Coumarins/src\n",
      "['Prostate' 'Leukemia/Lymphoma' 'Breast' 'Colon' 'Glioma' 'Bone' 'Ovarian'\n",
      " 'Cervical' 'Liver' 'Gastric' 'Lung' 'Pancreatic' 'Oral' 'Renal'\n",
      " 'Cholangiocarcinoma' 'Salivary Gland' 'Skin' 'Melanoma']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor)\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score, make_scorer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Please Change the paths to match your own enviroment.\n",
    "\n",
    "DATA_PATH = '../Total_Data.csv'\n",
    "RESULT_DIR = '../GridSearchingResults/Models/'\n",
    "REPORT_DIR = '../GridSearchingReports/Reports/'\n",
    "\n",
    "data = pd.read_csv(DATA_PATH);\n",
    "\n",
    "data = data[['Cancer Type', 'Coumarin Type', 'Coumarin Dose', 'Time', 'Viability']].dropna();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
